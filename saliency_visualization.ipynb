{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization using NiFTi files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter Notebook file demonstrates the visualization of the saliency maps of an CMR, given a pretrained binary quality classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as mpl_color_map\n",
    "from PIL import Image\n",
    "import nibabel as nib\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.set_printoptions(threshold=5000)\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from ipywidgets import interact\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from utils.custom_gradcam import GradCam\n",
    "from utils.custom_guided_backprop import GuidedBackprop\n",
    "from utils.custom_guided_gradcam import guided_grad_cam\n",
    "from utils.misc_functions import (DictAsMember, apply_colormap_on_image,\n",
    "                                  get_positive_negative_saliency, convert_to_grayscale)\n",
    "from models.models import get_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is necessary to provide the name of the model, the path for the weights, and the path for the NIfTI image file. For instance, here, `IMAGE` corresponds to a sample testing image with the LVOT region while `IMAGE2` does not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_NAME = \"alexnet\"\n",
    "MODEL_NAME = \"resnet50\"\n",
    "MODEL_PARAMS_PATH = \"resnet50_model.pth\"\n",
    "IMAGE = \"example_data/patient019_4d.nii.gz\" # Good Quality Image\n",
    "IMAGE2 = \"example_data/XXXXX\" # Image with Motion Artefacts\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "n_classes = 2  # Good Quality vs Img w/ Motion Artefact\n",
    "MODEL_ARGS = {\"name\": MODEL_NAME,\n",
    "             \"last_layer\": \"layer4\",\n",
    "             \"last_block\": 2}\n",
    "MODEL_ARGS = DictAsMember(MODEL_ARGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images are opened using the loader function in `nibabel` package. Normalization and dtype conversion are also applied at this step. The argument of NIfTI Image loader can be changed from `IMAGE` to `IMAGE2` in order to continue testing with an image with motion artefacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the image: (256, 216, 11, 30)\n"
     ]
    }
   ],
   "source": [
    "proxy_img = nib.load(IMAGE)\n",
    "img = proxy_img.get_fdata()\n",
    "\n",
    "img = (img / img.max() * 255).astype(np.uint8).squeeze().transpose(1, 0, 2, 3)\n",
    "img = img[:, :, :, :]\n",
    "print(\"Shape of the image: {}\".format(img.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4-D Visualization of the image can be viewed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(d=0, t=0, **kwargs):\n",
    "    d = int(d)\n",
    "    t = int(t)\n",
    "    img_rgb = img[:, :, d, t]\n",
    "    plt.subplots(figsize=(5, 5))\n",
    "    io.imshow(img_rgb)\n",
    "\n",
    "def player(image):\n",
    "    play = widgets.Play(\n",
    "        value=0,\n",
    "        min=0,\n",
    "        max=img.shape[3]-1,\n",
    "        step=1,\n",
    "        interval=100,\n",
    "        description=\"Press play\",\n",
    "        disabled=False\n",
    "    )\n",
    "\n",
    "    depth_slider = widgets.FloatSlider(min=0, max=image.shape[2]-1, step=1)\n",
    "    time_slider = widgets.FloatSlider(min=0, max=image.shape[3]-1, step=1)\n",
    "\n",
    "    widgets.jslink((play, 'value'), (time_slider, 'value'))\n",
    "    widgets.HBox([play, time_slider])\n",
    "\n",
    "    interact(update,\n",
    "             d=depth_slider,\n",
    "             t2=play,\n",
    "             t=time_slider,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f04bbaaa6c184dadb4a50d5ccb5b9139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='d', max=10.0, step=1.0), FloatSlider(value=0.0, desc…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "player(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the transformations prior processing the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = (224, 224)\n",
    "img_mean = [0.485, 0.456, 0.406]\n",
    "img_std = [0.229, 0.224, 0.225]\n",
    "data_transforms = transforms.Compose([transforms.Resize(input_size),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize(mean=img_mean,\n",
    "                                                           std=img_std)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the model and loading its parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "model = get_model(MODEL_NAME,\n",
    "                  device,\n",
    "                  pretrained=False,\n",
    "                  n_classes=n_classes)\n",
    "model.load_state_dict(torch.load(MODEL_PARAMS_PATH, map_location=device)[\"model\"])\n",
    "model.eval()\n",
    "print(\"Model loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a loop over depth and phase axes in order to process the images slice-by-slice. During the loop, we traverse over these image slices and collect the predictions, GradCAM maps, Guided Backpropagation maps and Guided GradCAM maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "probs = []\n",
    "\n",
    "resized_image = np.zeros((224, 224, *img.shape[2:]))\n",
    "gradcam_map = np.zeros((224, 224, *img.shape[2:]))\n",
    "pos_gbp_map = np.zeros((224, 224, *img.shape[2:]))\n",
    "neg_gbp_map = np.zeros((224, 224, *img.shape[2:]))\n",
    "ggradcam_map = np.zeros((224, 224, *img.shape[2:]))\n",
    "\n",
    "gcv2 = GradCam(model, MODEL_ARGS)\n",
    "gbp = GuidedBackprop(model, MODEL_NAME)\n",
    "softmax = nn.Softmax(dim=1)\n",
    "color_map = mpl_color_map.get_cmap(\"hsv\")\n",
    "\n",
    "# with torch.no_grad():\n",
    "for d in range(img.shape[2]):\n",
    "    for t in range(img.shape[3]):\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Convert BW to Pseudo-RGB Image\n",
    "        slice = Image.fromarray(img[:, :, None, d, t].repeat(3, axis=2))\n",
    "        resized_image[:, :, d, t] = slice.resize((224, 224)).convert(\"L\")\n",
    "\n",
    "        # Conversion to PIL Image and applying PyTorch transforms\n",
    "#         proc_img = Image.fromarray(slice)\n",
    "        proc_img = data_transforms(slice)\n",
    "        proc_img = torch.unsqueeze(proc_img, dim=0).to(device, dtype=torch.float32)\n",
    "        proc_img.requires_grad = True\n",
    "\n",
    "        # Forward Pass\n",
    "        output = model(proc_img)\n",
    "        prob = softmax(output.detach())\n",
    "        _, pred = torch.max(output.detach(), 1)\n",
    "\n",
    "        slice = slice.resize((224, 224))\n",
    "\n",
    "        # GradCAM results\n",
    "        cam, _ = gcv2.generate_cam(proc_img, torch.argmax(pred))\n",
    "        cam_p, cam_on_image = apply_colormap_on_image(slice, cam, 'hsv')\n",
    "        gradcam_map[:, :, d, t] = cam_p.convert(\"L\")   \n",
    "        \n",
    "        # Guided BackPropagation Results\n",
    "        guided_grads = gbp.generate_gradients(proc_img, torch.argmax(pred))\n",
    "        psal, nsal = get_positive_negative_saliency(guided_grads)\n",
    "        psal = convert_to_grayscale(psal)\n",
    "        nsal = convert_to_grayscale(nsal)\n",
    "        psal = psal - psal.min()\n",
    "        psal /= (psal.max() - psal.min())\n",
    "        nsal = nsal - nsal.min()\n",
    "        nsal /= (nsal.max() - nsal.min())\n",
    "        pos_gbp_map[:, :, d, t] = psal\n",
    "        neg_gbp_map[:, :, d, t] = nsal\n",
    "        \n",
    "        # Guided Grad-CAM Results\n",
    "        guided_gradcams = guided_grad_cam(cam, guided_grads)\n",
    "        bw_guided_gradcams = convert_to_grayscale(guided_gradcams)\n",
    "        ggradcam_map[:, :, d, t] = bw_guided_gradcams\n",
    "        \n",
    "        preds.append(pred)\n",
    "        probs.append(prob)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we obtained all of our saliency maps of interests, we can now write a new update function to visualize their change over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(d=0, t=0, **kwargs):\n",
    "    d = int(d)\n",
    "    t = int(t)\n",
    "    img_rgb = resized_image[:, :, d, t]\n",
    "    gradcam_rgb = gradcam_map[:, :, d, t]\n",
    "    pos_gbp_rgb = pos_gbp_map[:, :, d, t]\n",
    "    neg_gbp_rgb = neg_gbp_map[:, :, d, t]\n",
    "    ggradcam_rgb = ggradcam_map[:, :, d, t]\n",
    "    \n",
    "    plt.subplots(1, 5, figsize=(20, 5))\n",
    "    plt.subplot(1, 4, 1)\n",
    "    io.imshow(img_rgb, cmap=\"gray\")\n",
    "    plt.subplot(1, 4, 2)\n",
    "    io.imshow(gradcam_rgb, cmap=\"Reds\")\n",
    "    plt.subplot(1, 4, 3)\n",
    "    io.imshow(pos_gbp_rgb, cmap=\"Reds\")\n",
    "    plt.subplot(1, 4, 4)\n",
    "    io.imshow(ggradcam_rgb, cmap=\"Reds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cd06c11b61f47e0bdd8eb9bd1fc8552",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='d', max=10.0, step=1.0), FloatSlider(value=0.0, desc…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "player(gradcam_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we can obtain the overall prediction by averaging the predictions of all slices. A value close to $1$ demonstrate that there is no motion artefact, while a value close to $0$ show that the image volume consists of motion artefacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average prediction: tensor([1.0000], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(\"Average prediction: {}\".format(sum(preds).float()/len(preds)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
